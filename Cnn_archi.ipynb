{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CNN ARCHITECTURE"
      ],
      "metadata": {
        "id": "WUzNO1aprdoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Filters (Kernels)\n",
        "\n",
        "    Filters are small matrices of learnable parameters (weights) that slide over the input image or feature map during the convolution operation. Each filter is designed to detect a specific type of pattern or feature in the input.\n",
        "\n",
        "    In early layers, filters typically learn low-level features such as edges, corners, and textures.\n",
        "\n",
        "    In deeper layers, filters learn high-level features such as shapes, object parts, or complex patterns.\n",
        "\n",
        "    Filters share weights across the entire input, which significantly reduces the number of parameters compared to fully connected layers.\n",
        "\n",
        "    Feature Maps\n",
        "\n",
        "    A feature map is the output produced after applying a filter to the input through convolution and an activation function.\n",
        "\n",
        "    Each filter generates one feature map.\n",
        "\n",
        "    Feature maps highlight the presence and spatial location of learned features.\n",
        "\n",
        "    Multiple filters produce multiple feature maps, allowing the CNN to capture diverse features simultaneously.\n",
        "\n",
        "    In summary: Filters are feature detectors, and feature maps are the visual representations of detected features at different spatial locations.\n",
        "\n",
        "2.Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Padding\n",
        "\n",
        "    Padding refers to adding extra pixels (usually zeros) around the border of the input image.\n",
        "\n",
        "    Types of padding:\n",
        "\n",
        "    Valid padding: No padding is added. Output size shrinks after convolution.\n",
        "\n",
        "    Same padding: Padding is added so that the output size remains the same as the input size (when stride = 1).\n",
        "\n",
        "    Benefits of padding:\n",
        "\n",
        "    Preserves spatial dimensions\n",
        "\n",
        "    Prevents loss of edge information\n",
        "\n",
        "    Allows deeper networks without rapid shrinking of feature maps\n",
        "\n",
        "    Stride\n",
        "\n",
        "    Stride is the number of pixels by which the filter moves across the input.\n",
        "\n",
        "    Stride = 1: Filter moves one pixel at a time (more detailed output)\n",
        "\n",
        "    Stride > 1: Filter skips pixels (downsampling effect)\n",
        "\n",
        "    Output Dimension Formula\n",
        "\n",
        "    For an input of size ğ‘, filter size ğ¹, padding ğ‘ƒ and stride ğ‘†\n",
        "    \n",
        "    Output=(ğ‘âˆ’ğ¹+2ğ‘ƒ)ğ‘†+1\n",
        "\n",
        "\tâ€‹Effect summary:\n",
        "\n",
        "    Increasing padding increases output size\n",
        "\n",
        "    Increasing stride decreases output size\n",
        "\n",
        "3.Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Receptive Field\n",
        "\n",
        "    The receptive field is the region of the input image that influences a particular neuron or feature in a CNN layer.\n",
        "\n",
        "    In early layers, the receptive field is small and local.\n",
        "\n",
        "    In deeper layers, the receptive field grows and covers a larger portion of the input.\n",
        "\n",
        "    Importance\n",
        "\n",
        "    Enables CNNs to capture contextual and global information\n",
        "\n",
        "    Larger receptive fields help recognize complex patterns and objects\n",
        "\n",
        "    Deep architectures combine multiple small receptive fields to effectively model large structures\n",
        "\n",
        "    Thus, deep CNNs can understand both fine details and global structures.\n",
        "\n",
        "\n",
        "4.Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.\n",
        "    \n",
        "ans-\n",
        "\n",
        "    Filter Size\n",
        "\n",
        "    The number of parameters in a convolutional layer is given by:\n",
        "    (ğ¹ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ ğ»ğ‘’ğ‘–ğ‘”â„ğ‘¡ Ã— ğ¹ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ ğ‘Šğ‘–ğ‘‘ğ‘¡â„ Ã— ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ ğ¶â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ğ‘ ) Ã—ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ¹ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿğ‘  +ğµğ‘–ğ‘ğ‘ \n",
        "\n",
        "    Larger filters increase the number of parameters\n",
        "\n",
        "    Smaller filters (e.g., 3Ã—3) are preferred as they:\n",
        "\n",
        "    Reduce parameters\n",
        "\n",
        "    Improve computational efficiency\n",
        "\n",
        "    Allow deeper networks (used in VGG)\n",
        "\n",
        "    Stride\n",
        "\n",
        "    Stride does not directly affect the number of parameters\n",
        "\n",
        "    It affects the output size, which impacts computation and memory usage\n",
        "\n",
        "    Conclusion: Filter size controls parameter count, while stride controls spatial resolution.\n",
        "\n",
        "5.Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    Feature\t   LeNet\t              AlexNet\t                 VGG\n",
        "    Year\t   1998\t                  2012\t                     2014\n",
        "    Depth\t   Shallow (5 layers)\t Medium (8 layers)\t     Deep (16â€“19 layers)\n",
        "    Filter Sizes Large (5Ã—5)\t   Mixed (11Ã—11, 5Ã—5, 3Ã—3)\t     Small (3Ã—3)\n",
        "    Activation\tTanh\t                   ReLU\t                 ReLU\n",
        "    Dataset\t    MNIST\t                 ImageNet\t            ImageNet\n",
        "    Performance\tLow\t                     High\t               Very High\n",
        "\n",
        "    LeNet: Simple and efficient, used for digit recognition\n",
        "\n",
        "    AlexNet: Introduced ReLU, dropout, and GPU training\n",
        "\n",
        "    VGG: Uniform architecture with small filters and deep layers\n",
        "    "
      ],
      "metadata": {
        "id": "C9yYfRGdrmdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n"
      ],
      "metadata": {
        "id": "gLDCsOQSyXrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Preprocess\n",
        "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "MaxPooling2D((2,2)),\n",
        "Flatten(),\n",
        "Dense(128, activation='relu'),\n",
        "Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64)\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ais8_5jyb_G",
        "outputId": "029195ed-c2a3-4c04-da7f-7f19a088df2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 37ms/step - accuracy: 0.8961 - loss: 0.3560\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 34ms/step - accuracy: 0.9813 - loss: 0.0614\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9889 - loss: 0.0372\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 35ms/step - accuracy: 0.9930 - loss: 0.0245\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9951 - loss: 0.0165\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9824 - loss: 0.0536\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04604960232973099, 0.9843000173568726]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture."
      ],
      "metadata": {
        "id": "6vnbk1VDzfm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "MaxPooling2D((2,2)),\n",
        "Conv2D(64, (3,3), activation='relu'),\n",
        "MaxPooling2D((2,2)),\n",
        "Flatten(),\n",
        "Dense(128, activation='relu'),\n",
        "Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnKTDMDzzkXr",
        "outputId": "aad115c3-b2c7-4dc2-b2c2-d963ca6b28af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 83ms/step - accuracy: 0.3781 - loss: 1.7023\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 81ms/step - accuracy: 0.5994 - loss: 1.1455\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 81ms/step - accuracy: 0.6518 - loss: 1.0031\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 79ms/step - accuracy: 0.6899 - loss: 0.8961\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 80ms/step - accuracy: 0.7178 - loss: 0.8139\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - accuracy: 0.7439 - loss: 0.7386\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 79ms/step - accuracy: 0.7658 - loss: 0.6747\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 80ms/step - accuracy: 0.7892 - loss: 0.6108\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 83ms/step - accuracy: 0.8035 - loss: 0.5572\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 80ms/step - accuracy: 0.8212 - loss: 0.5190\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.7108 - loss: 0.9184\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9239018559455872, 0.7074000239372253]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation"
      ],
      "metadata": {
        "id": "ahlciy242q6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "transforms_mnist = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, transform=transforms_mnist, download=True)\n",
        "test_data = datasets.MNIST(root='./data', train=False, transform=transforms_mnist)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # Added padding to maintain size\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 14 * 14, 128) # Adjusted linear layer input size due to padding and pooling\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "epochs = 5\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluation Loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on the test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klfoADET2vxC",
        "outputId": "ef87511b-51b1-46f8-afac-c706a2103342"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/5, Loss: 0.1077\n",
            "Epoch 2/5, Loss: 0.0165\n",
            "Epoch 3/5, Loss: 0.0094\n",
            "Epoch 4/5, Loss: 0.0856\n",
            "Epoch 5/5, Loss: 0.0659\n",
            "Training finished.\n",
            "Accuracy on the test set: 98.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model.\n"
      ],
      "metadata": {
        "id": "VLc-Etai3die"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7b00288",
        "outputId": "41cc692b-ad52-4b7b-b8c9-fca3727d2cd4"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(train_data.num_classes, activation='softmax') # Output layer for number of classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# Use steps_per_epoch and validation_steps based on your dataset size\n",
        "# For dummy data, these numbers are illustrative.\n",
        "model.fit(\n",
        "    train_data,\n",
        "    epochs=3, # Reduced epochs for quick demonstration\n",
        "    validation_data=validation_data, # Use validation_data for evaluation during training\n",
        "    steps_per_epoch=train_data.samples // train_data.batch_size,\n",
        "    validation_steps=validation_data.samples // validation_data.batch_size\n",
        ")\n",
        "\n",
        "# Evaluate the model (you would typically use a separate test_data generator for this)\n",
        "# For dummy data, we'll just evaluate on the 'validation_data' which is pointing to 'train' for now\n",
        "loss, accuracy = model.evaluate(validation_data, steps=validation_data.samples // validation_data.batch_size)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.7031"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 322 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x79c172dc1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.5000 - loss: 0.7031 - val_accuracy: 0.5000 - val_loss: 26.7193\n",
            "Epoch 2/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 26.7193 - val_accuracy: 0.5000 - val_loss: 2.9460\n",
            "Epoch 3/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 2.9460 - val_accuracy: 0.9000 - val_loss: 0.3424\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.9000 - loss: 0.3424\n",
            "Test Loss: 0.3424, Test Accuracy: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into â€œNormalâ€\n",
        "and â€œPneumoniaâ€ categories. Describe your end-to-end approachâ€“from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit"
      ],
      "metadata": {
        "id": "kBwS3rZXKuxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans-\n",
        "\n",
        "    End-to-End CNN for Medical Imaging Web App\n",
        "\n",
        "    Data Preparation\n",
        "\n",
        "    Collect and label chest X-ray images\n",
        "\n",
        "    Resize and normalize images\n",
        "\n",
        "    Split into train, validation, and test sets\n",
        "\n",
        "    Model Training\n",
        "\n",
        "    Use CNN or transfer learning (ResNet/VGG)\n",
        "\n",
        "    Apply data augmentation\n",
        "\n",
        "    Optimize using Adam and cross-entropy loss\n",
        "\n",
        "    Evaluation\n",
        "\n",
        "    Use accuracy, precision, recall, and F1-score\n",
        "\n",
        "    Validate on unseen test data\n",
        "\n",
        "    Deployment\n",
        "\n",
        "    Save trained model (.h5 or .pt)\n",
        "\n",
        "    Build Streamlit UI for image upload\n",
        "\n",
        "    Load model and display predictions\n",
        "\n",
        "    Maintenance\n",
        "\n",
        "    Monitor performance\n",
        "\n",
        "    Retrain with new data"
      ],
      "metadata": {
        "id": "pNxd0urcK0CN"
      }
    }
  ]
}